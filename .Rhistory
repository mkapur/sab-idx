labs(title = 'AK_trawl_GOA', subtitle="Subset race_cpue_by_haul.csv to GOA, Depth < 700 and drop 1984, 1987")
## Create timeblocking
GOA_late <- subset(Data4, Year > 1993) %>% mutate(Survey = 'GOA_late', Region = "GOA")
GOA_early <- subset(Data4, Year <= 1993) %>% mutate(Survey = 'GOA_early', Region = "GOA")
Data_CPUE = rbind( Data_CPUE, GOA_early, GOA_late )
rm(Data4)
}
# Load GOA -- these already have zeros,
## Survey_catch_analysis.csv has the catch weight by station/haul as estimated
## by l-w relationship. It only has this for the domestic survey and will have to be
##  crossreferenced to station locations from one of the other data sources.
if( "AK_DOM_LL" %in% Surveys_to_include ){
## slow to make the merge; did once and save.
AK_DOM_LL_Loc <-  read.csv( paste0(DataFile,"AK/LLData/catch_summary_view_with_nulls.csv"), header=TRUE,skip = 6) %>%
filter(Year > 1989)
## these have a small fudge factor leading to many dupes. get a mean survey location for each.
AK_DOM_LL0 <-  read.csv( paste0(DataFile,"AK/LLData/SurveyCatchAnalysis.csv"), header = TRUE)
names(AK_DOM_LL0)[1] <- 'Year'
# ## get station numbers from dom LL
AK_DOM_LL <- AK_DOM_LL_Loc %>% group_by(Station.Number,Year) %>% dplyr::summarise(meanLat = mean(Start.Latitude..DD.),
meanLon = mean(Start.Longitude..DD.)) %>%
select(Station.Number,Year, meanLat, meanLon) %>%
merge(AK_DOM_LL0,.,
by = c("Year",'Station.Number'),  all.y = TRUE)
# ## overwrite NA weights to zero
AK_DOM_LL$Total.Weight..kg.[is.na( AK_DOM_LL$Total.Weight..kg.)] <- 0
write.csv(AK_DOM_LL, file = paste0(DataFile,"AK/LLData/merged_AK_DOM_LL.csv") )
Data5 <- read.csv(paste0(DataFile,"AK/LLData/merged_AK_DOM_LL.csv") )%>%
mutate(AreaSwept_km2 = 0.01) %>%
select(c('Year','meanLat','meanLon','AreaSwept_km2','Total.Weight..kg.'))
# Data5 = FishData::add_missing_zeros( data_frame=GOA, Method=ZeroMethod, if_multiple_records=ifelse(ZeroMethod=="Slow",'Error','Combine'),
# unique_sample_ID_colname="TowID", sample_colname="WEIGHT..KG.", species_subset=Species, species_colname="COMMON.NAME" )
Data5 <- ThorsonUtilities::rename_columns( Data5[,c('Year','meanLat','meanLon','AreaSwept_km2','Total.Weight..kg.')],
newname=c('Year','Lat','Lon','AreaSwept_km2','Catch_KG'))  %>%
mutate(Survey = 'AK_DOM_LL', Region = 'AK')
# Data5  cbind("Survey"="GOA", "Region"="GOA", Data5)
Data5$Lon <- ifelse(  Data5$Lon > 0,   Data5$Lon*-1,  Data5$Lon) ## otherwise things in japan
# Data5 %>% group_by(Year) %>% summarise(sum(Catch_KG ==0), n(), length(unique(Lat))) %>% tail(10)
Data5 %>%
mutate(ZERO_CATCH = (Catch_KG == 0))%>%
ggplot(., aes(y = Lat, x = Lon, color = ZERO_CATCH)) + geom_point(alpha = 0.2) + facet_wrap(~Year) +
labs(title = 'AK_DOM_LL', subtitle="Station Numbers from catch_summary_view_with_nulls")
# AK_DOM_late <- subset(Data5, YEAR > 1993) %>% mutate(Survey = 'AK_DOM_late', "Region = AK")
# AK_DOM_early <- subset(Data5, YEAR <= 1993) %>% mutate(Survey = 'AK_DOM_early', "Region = AK")
Data_CPUE <- rbind( Data_CPUE, Data5) #AK_DOM_early, AK_DOM_late )
rm(Data5)
# ggplot(Data4, aes(x = Lon, y = Lat, col = Catch_KG)) + geom_point() + facet_wrap(~Year)
}
# Load EBS -- these already have zeros
if( "EBS" %in% Surveys_to_include ){
EBS <- read.csv( paste0(DataFile,"AK/race_cpue_by_haul.csv"), header=TRUE ) %>%
filter(Survey == 'EBS_SHELF'  | Survey == 'EBS_SLOPE' | Survey == 'AI')
names(EBS) <- toupper(names(EBS))
# EBS$WEIGHT..KG. <-  EBS$WEIGHT..KG.*10
EBS <- EBS %>%
mutate(TowID=paste(EBS[,'YEAR'],EBS[,'STARTING.LATITUDE..DD.'],EBS[,'STARTING.LONGITUDE..DD.'],sep=""),
AreaSwept_km2=EFFORT..KM2.) %>%
select("TowID", "SURVEY", "COMMON.NAME", 'YEAR','STARTING.LATITUDE..DD.','STARTING.LONGITUDE..DD.','AreaSwept_km2','WEIGHT..KG.')#0.01
# Data6 <- EBS
# Data6 <- FishData::add_missing_zeros( data_frame=EBS, Method='Slow', if_multiple_records=ifelse(ZeroMethod=="Slow",'Error','Combine'),
# unique_sample_ID_colname="TowID", sample_colname="WEIGHT..KG.", species_subset=Species, species_colname="COMMON.NAME" )
Data6 <- ThorsonUtilities::rename_columns( EBS[,c('TowID',"SURVEY", 'YEAR','STARTING.LATITUDE..DD.','STARTING.LONGITUDE..DD.','AreaSwept_km2','WEIGHT..KG.')],
newname=c('TowID',"Survey", 'Year','Lat','Lon','AreaSwept_km2','Catch_KG')) #%>%
#   complete(TowID,nesting(Survey, Year,Lat,Lon,AreaSwept_km2),
#            fill = list(Catch_KG = 0.0)) %>%
#   select(-TowID)
Data6$Lon <- ifelse(  Data6$Lon > 0,   Data6$Lon*-1,  Data6$Lon) ## otherwise things in japan
# Data6 %>% group_by(Year) %>% summarise(sum(Catch_KG ==0), n(), length(unique(Lat)))
#   Data6 %>%
# mutate(ZERO_CATCH = (Catch_KG == 0))%>%
# ggplot(., aes(y = Lat, x = Lon, color = ZERO_CATCH)) + geom_point(alpha = 0.2) + facet_wrap(~Year) +
#     labs(title = 'AK_TRAWL_EBS', subtitle = "Subset to EBS_SHELF, EBS_SLOPE & AI")
## Create timeblocking
EBS_late <- subset(Data6, Year > 1993) %>% mutate(Survey = 'EBS_late', Region = "EBS") %>% select(-TowID)
EBS_early <- subset(Data6, Year <= 1993) %>% mutate(Survey = 'EBS_early', Region = "EBS") %>% select(-TowID)
Data_CPUE <- rbind( Data_CPUE, EBS_late, EBS_early  )
}
# Memory management
gc()
# Restrict years
Data_Geostat <- Data_CPUE[which(Data_CPUE$Year>=Year_Range[1] & Data_CPUE$Year<=Year_Range[2]),]
save(Data_Geostat, file = paste0(DateFile,"/Data_Geostat.Rdata"))
Data_Geostat %>% group_by(Survey) %>% dplyr::summarise(min(Year))
Data_Geostat <- na.omit( Data_Geostat )
Data_Geostat %>% group_by(Survey) %>% dplyr::summarise(min(Catch_KG), max(Catch_KG)) ## should be 0 to pos
Data_Geostat %>% group_by(Survey) %>% dplyr::summarise(min(AreaSwept_km2), max(AreaSwept_km2)) ## should be positive
Data_Geostat %>% group_by(Survey) %>% dplyr::summarise(min(Year)) ## should be > Year_Range[1]
##  Define regions ----
# Including two different approaches
Region <- NULL
if( TRUE ){
if(any(c("WCGBTS","Triennial") %in% Surveys_to_include)) Region = c( Region, "California_current")
if("BCs" %in% Surveys_to_include | "BCt" %in% Surveys_to_include) Region = c( Region, "British_Columbia" )
if("GOA" %in% Surveys_to_include) Region = c( Region, "Gulf_of_Alaska" )
if("EBS" %in% Surveys_to_include) Region = c( Region, "Eastern_Bering_Sea" )
Extrapolation_List <- make_extrapolation_info( Region=Region, strata_to_use=c('SOG','WCVI','QCS','HS','WCHG'),
zone=Zone, create_strata_per_region=create_strata_per_region )
}else{
if(any(c("WCGBTS","Triennial") %in% Surveys_to_include)) Region = c( Region, "California_current")
if("BCs" %in% Surveys_to_include | "BCt" %in% Surveys_to_include) Region = c( Region, "British_Columbia" )
if("GOA" %in% Surveys_to_include) Region = c( Region, "Gulf_of_Alaska" )
if("EBS" %in% Surveys_to_include) Region = c( Region, "Eastern_Bering_Sea" )
observations_LL <- Data_Geostat[ which(Data_Geostat[,'Region']=="BC"), c('Lat','Lon') ]
Extrapolation_List <-  make_extrapolation_info( Region=Region,
observations_LL=observations_LL, zone=Zone, create_strata_per_region=create_strata_per_region )
}
## Make spatial list ----
Spatial_List <- make_spatial_info( n_x=n_x, Lon=Data_Geostat[,'Lon'],
Lat=Data_Geostat[,'Lat'],
Extrapolation_List=Extrapolation_List,
DirPath=DateFile, Save_Results=FALSE,
"knot_method"="grid", refine=FALSE,
fine_scale=fine_scale )
# Plot details
MapDetails_List <- make_map_info( "Region"="Other",
"spatial_list"=Spatial_List,
"Extrapolation_List"=Extrapolation_List )
Year_Set <- min(Data_Geostat[,'Year']):max(Data_Geostat[,'Year'])
# Exclude surveys without any encounters
EncNum_k <- tapply( Data_Geostat[,'Catch_KG'], INDEX=Data_Geostat[,'Survey'],
FUN=function(vec){sum(vec>0)} )
if( any(EncNum_k==0) ){
Which = which( EncNum_k==0 )
Which2Remove = which( Data_Geostat[,'Survey'] %in% names(Which) )
Data_Geostat = Data_Geostat[-Which2Remove,]
Spatial_List$loc_i = Spatial_List$loc_i[-Which2Remove,]
Spatial_List$knot_i = Spatial_List$knot_i[-Which2Remove]
Data_Geostat[,'Survey'] = droplevels( Data_Geostat[,'Survey'] )
}
# Make catchability matrix (Q_i) -- this should be where the BC one gets separated. The resulting Q_ik will have n-1 columns, with baseQ excluded.
if( length(unique(Data_Geostat[,'Survey']))==1 ){
Q_ik <- matrix(0, ncol=1, nrow=nrow(Data_Geostat))
}else{
Q_ik <- ThorsonUtilities::vector_to_design_matrix( Data_Geostat[,'Survey'] )
if( !(BaseQ %in% colnames(Q_ik)) ) stop("Problem with Q_ik")
Q_ik <- Q_ik[,-which(colnames(Q_ik)==BaseQ),drop=FALSE]
}
# Plot location of data
png(paste0(DateFile,"/Extrapolation_List.png"), width = 8, height = 6, units = 'in', res = 520)
plot( Extrapolation_List )
dev.off()
png(paste0(DateFile,"/Spatial_List.png"), width = 8, height = 6, units = 'in', res = 520)
plot( Spatial_List )
dev.off()
# Make and Run TMB model ----
# (THIS WILL BE SIMILAR FOR EVERY DATA SET)
# # Make TMB data list
# TmbData <-
#   make_data(
#     "FieldConfig" = FieldConfig,
#     "RhoConfig" = RhoConfig,
#     "ObsModel" = ObsModel,
#     "c_i" = rep(0, nrow(Data_Geostat)),
#     "b_i" = Data_Geostat[, 'Catch_KG'],
#     "a_i" = Data_Geostat[, 'AreaSwept_km2'],
#     "t_i" = Data_Geostat[, 'Year'],
#     "spatial_list" = Spatial_List,
#     "Q_ik" = Q_ik
#   )
## from CC version
TmbData <- VAST::make_data(#"X_itp"=X_itp,
#"X_gtp"=X_gtp,
#"Xconfig_zcp"=Xconfig_zcp,
"Version"=Version,
"Aniso"=Aniso,
"FieldConfig"=FieldConfig,
"OverdispersionConfig" = c("Eta1"=0, "Eta2"=0),
"RhoConfig"=RhoConfig,
"ObsModel"= ObsModel,
"c_i"=rep(0,nrow(Data_Geostat)),
"b_i"=Data_Geostat[,'Catch_KG'],
"a_i"=Data_Geostat[,'AreaSwept_km2'],
# "v_i"=as.numeric(Data_Geostat[,'Vessel']),#-1,
"t_i"=Data_Geostat[,'Year'],
"Q_ik" = Q_ik,
"spatial_list"=Spatial_List,
"Options"=Options )
# Make TMB object
# TmbList <- make_model( "TmbData"=TmbData, "RhoConfig"=RhoConfig,
#                        "Use_REML"=Use_REML, "RunDir"=DateFile,
#
#                        "Version"=get_latest_version() )
TmbList <- make_model("build_model"=TRUE, "TmbData"=TmbData, "RunDir"=DateFile,
"Version"=Version, "RhoConfig"=RhoConfig,
"loc_x"=Spatial_List$loc_x, "Method"=Method, "TmbDir"=getwd())
# ## this is what gets called within make_model
# plist <- make_parameters("DataList"=TmbData, "RhoConfig"=RhoConfig,
#             Version=get_latest_version())
# #
# plist$lambda1_k
# Run model ----
Obj <- TmbList[["Obj"]]
Obj$par['lambda2_k'] ## should not be NA
Obj$par['lambda1_k']
# Obj$par['gamma1_k']
Opt <- TMBhelper::fit_tmb(
obj = Obj,
lower = TmbList[["Lower"]],
upper = TmbList[["Upper"]],
newtonsteps = 1,
getsd = TRUE,
bias.correct = TRUE,
bias.correct.control = list(vars_to_correct = "Index_cyl"),
savedir = DateFile
)  # , rel.tol=1e-20
Report <- TmbList$Obj$report()
ParHat <- TmbList$Obj$env$parList()
# Save stuff [NOTE OBJ IS INSIDE SAVE]
Save <- list("Opt"=Opt, "Report"=Report,
"ParHat"=TmbList$Obj$env$parList(Opt$par),
'Obj' = Obj)
save(Save, file=paste0(DateFile,"Save_original.RData"))
# strata.limits <- subset(strata.limits.full, STRATA %in% as.factor(strata.limits.input))
#
# Obj$report()
#
# Obj$report(Obj$env$last.par.best) ## all derived quantities
#
################
# Make diagnostic plots ----
# Plot locations
plot_data( Extrapolation_List=Extrapolation_List, Spatial_List=Spatial_List,
Data_Geostat=Data_Geostat, PlotDir=DateFile,
Plot1_name="Data_and_knots.png", Plot2_name="Data_by_year.png", col="red")
# Plot index
Index <- plot_biomass_index( DirName=DateFile,
TmbData=TmbData,
use_biascorr = TRUE,
Sdreport=Opt$SD,
Year_Set=Year_Set,
strata_names=c("All",Region),
plot_log=TRUE, width=6, height=6 ) # , total_area_km2=sum(a_xl[,1])
plot_range_index( Sdreport=Opt$SD, Report=Report, Year_Set=Year_Set, TmbData=TmbData,
Znames=colnames(TmbData$Z_xm), PlotDir=DateFile )
# Plot Anisotropy
plot_anisotropy( FileName=paste0(DateFile,"Aniso.png"), Report=Report )
# Plot encounter rate diagnostics
# plot_quantile_diagnostic( Report=Report, TmbData=TmbData, DateFile=DateFile)
# Positive catch rate diagnostics
Q <- plot_quantile_diagnostic( TmbData=TmbData, Report=Report, DateFile=DateFile ) # SpatialDeltaGLMM::
# Pearson residuals diagnostics
plot_residuals( Lat_i=Data_Geostat[,'Lat'], Lon_i=Data_Geostat[,'Lon'],
extrapolation_list = Extrapolation_List,
TmbData=TmbData, Report=Report, Q=Q, savedir=DateFile, spatial_list=Spatial_List )
# Plot density
plot_maps( plot_set=3, Report=Report, PlotDF=MapDetails_List[["PlotDF"]],
working_dir=DateFile, Year_Set=Year_Set )
file.rename(paste0(DateFile,"/Table_for_SS3.csv"),
to = paste0(DateFile,"/Table_for_SS3_original.csv"))
file.rename(paste0(DateFile,"/parameter_estimates.txt"),
to = paste0(DateFile,"/parameter_estimates_original.txt"))
file.rename(paste0(DateFile,"/Index-Biomass.png"),
to = paste0(DateFile,"/Index-Biomass-original.png"))
file.rename(paste0(DateFile,"/density.png"),
to = paste0(DateFile,"/density-original.png"))
file.rename(paste0(DateFile,"/aniso.png"),
to = paste0(DateFile,"/aniso-original.png"))
file.rename(paste0(DateFile,"/Record.Rdata"),
to = paste0(DateFile,"/Record-Original.Rdata"))
# source("./R/reVAST_newstrata.R")
DateFile
DateFile = "C:/Users/Maia Kapur/Dropbox/UW/sab-idx/runs/2019-11-26_nx=250_Species=SAB_Triennial_WCGBTS_BCt/"
comp.name
comp.name = 'Maia Kapur'
assc <- read.csv("./data/assessment_CPUE.csv") %>%
mutate(Source = 'assessment', Estimate_metric_tons = Value ,
Fleet2 = substr(Index,1,2)) %>%
filter(Type == 'Abundance' |Type == 'Biomass')  %>%
filter(!grepl('PUE', Index),!grepl('NUM', Index)) %>%
bind_rows(.,
read.csv(paste0(DataFile,"/BC/BC_sable_survey_data.Aug262019.csv"))  %>%
filter(START_LONGITUDE <= 0 & !is.na(CPUE_TRAPS) & !is.na(TOTAL_SABLE_WEIGHT) &
SABLE_SET_TYPE == 'StRS') %>% group_by(SET_YEAR) %>%
dplyr::summarise(meanCPUE = mean(cpue)) %>%
mutate(Year = SET_YEAR, Value = meanCPUE, CV = NA,
Index = 'Filter_StRS', Type = 'Biomass', Source = 'Assessment',
Estimate_metric_tons =meanCPUE, Fleet2 =
'BC') %>%
select(-meanCPUE, -SET_YEAR))
## Correct scales
assc$Estimate_metric_tons[assc$Fleet2 == 'AK'] <- assc$Value[assc$Fleet2 == 'AK']*  1000
# assc$Estimate_metric_tons[assc$Fleet2 == 'WC'] <- assc$Value * 1000
assc$Estimate_metric_tons[assc$Fleet2 == 'BC'] <- assc$Value[assc$Fleet2 == 'BC'] * 1000
names(assc) <- c('Year','Value','SD_log',"Fleet","TYPE", 'Source',"Estimate_metric_tons","Fleet2")
require(dplyr)
require(ggplot2)
require(reshape2)
require(mapdata)
assc <- read.csv("./data/assessment_CPUE.csv") %>%
mutate(Source = 'assessment', Estimate_metric_tons = Value ,
Fleet2 = substr(Index,1,2)) %>%
filter(Type == 'Abundance' |Type == 'Biomass')  %>%
filter(!grepl('PUE', Index),!grepl('NUM', Index)) %>%
bind_rows(.,
read.csv(paste0(DataFile,"/BC/BC_sable_survey_data.Aug262019.csv"))  %>%
filter(START_LONGITUDE <= 0 & !is.na(CPUE_TRAPS) & !is.na(TOTAL_SABLE_WEIGHT) &
SABLE_SET_TYPE == 'StRS') %>% group_by(SET_YEAR) %>%
dplyr::summarise(meanCPUE = mean(cpue)) %>%
mutate(Year = SET_YEAR, Value = meanCPUE, CV = NA,
Index = 'Filter_StRS', Type = 'Biomass', Source = 'Assessment',
Estimate_metric_tons =meanCPUE, Fleet2 =
'BC') %>%
select(-meanCPUE, -SET_YEAR))
## Correct scales
assc$Estimate_metric_tons[assc$Fleet2 == 'AK'] <- assc$Value[assc$Fleet2 == 'AK']*  1000
# assc$Estimate_metric_tons[assc$Fleet2 == 'WC'] <- assc$Value * 1000
assc$Estimate_metric_tons[assc$Fleet2 == 'BC'] <- assc$Value[assc$Fleet2 == 'BC'] * 1000
names(assc) <- c('Year','Value','SD_log',"Fleet","TYPE", 'Source',"Estimate_metric_tons","Fleet2")
assc <- assc %>%select(Year, Fleet, Estimate_metric_tons, SD_log, TYPE, Source, Fleet2 ) %>%
mutate(uci=NA, lci = NA)
## see line 81 here for conv https://github.com/James-Thorson-NOAA/FishStatsUtils/blob/master/R/plot_index.R
vastc <- read.csv(paste0(DateFile,"/Table_for_SS3_original.csv")) %>%
mutate(TYPE = 'Abundance', Source = 'VAST',
lci = Estimate_metric_tons-SD_mt,
uci = Estimate_metric_tons+SD_mt) %>%
select(Year, Fleet, Estimate_metric_tons, SD_log, TYPE, Source, uci, lci ) %>%
filter(Fleet != 'Eastern_Bering_Sea')
vastc$Fleet2 <- NA
for(i in 1:nrow(vastc)){
vastc$Fleet2[i] <- ifelse(vastc$Fleet[i] == "California_current",
"WC",
ifelse(vastc$Fleet[i] == "British_Columbia", "BC",
"AK"))
if(vastc$Fleet[i] == 'All')   vastc$Fleet2[i] <- NA
}
## see line 81 here for conv https://github.com/James-Thorson-NOAA/FishStatsUtils/blob/master/R/plot_index.R
vastc <- read.csv(paste0(DateFile,"Table_for_SS3_original.csv")) %>%
mutate(TYPE = 'Abundance', Source = 'VAST',
lci = Estimate_metric_tons-SD_mt,
uci = Estimate_metric_tons+SD_mt) %>%
select(Year, Fleet, Estimate_metric_tons, SD_log, TYPE, Source, uci, lci ) %>%
filter(Fleet != 'Eastern_Bering_Sea')
paste0(DateFile,"Table_for_SS3_original.csv")
## see line 81 here for conv https://github.com/James-Thorson-NOAA/FishStatsUtils/blob/master/R/plot_index.R
vastc <- read.csv(paste0(DateFile,"Table_for_SS3.csv")) %>%
mutate(TYPE = 'Abundance', Source = 'VAST',
lci = Estimate_metric_tons-SD_mt,
uci = Estimate_metric_tons+SD_mt) %>%
select(Year, Fleet, Estimate_metric_tons, SD_log, TYPE, Source, uci, lci ) %>%
filter(Fleet != 'Eastern_Bering_Sea')
vastc$Fleet2 <- NA
for(i in 1:nrow(vastc)){
vastc$Fleet2[i] <- ifelse(vastc$Fleet[i] == "California_current",
"WC",
ifelse(vastc$Fleet[i] == "British_Columbia", "BC",
"AK"))
if(vastc$Fleet[i] == 'All')   vastc$Fleet2[i] <- NA
}
rbind(vastc,assc) %>%
filter(Fleet2 %in% c('WC','BC')) %>%
filter(Fleet %in% c("California_current","British_Columbia","Gulf_of_Alaska","Eastern_Bering_Sea",
'AK_DOM_LL','AK_GOA_TRW','Filter_StRS','BC_TRAP_SRV_STRAT','WC_TRI_TRW','WC_SH_SLP_TRW')) %>%
ggplot(., aes(x = Year, y =Estimate_metric_tons, col = Fleet, linetype = Fleet)) +
theme_minimal()+
# theme(panel.grid.minor = element_blank()) +
scale_x_continuous(limits = c(1975,2020)) +
# scale_color_brewer(palette = 'Spectral') +
scale_color_manual(values = c(rep('blue',3),'gold2','gold','seagreen2','seagreen3','seagreen4','grey22','grey44'))+
scale_linetype_manual(values = c(rep('solid',3), rep('dashed',40))) +
labs(x = 'Year', y = 'Estimate (mt)',
title = 'Indices from VAST and Assessment, by region') +
geom_line(lwd = 0.9)+
geom_ribbon(aes(ymin = lci, ymax = uci), alpha = 0.2, col = 'grey') +
# geom_point(pch = 1, cex = 3) +
facet_wrap(~Fleet2, scales = 'free_y', ncol = 3)
unique(assc$Fleet)
read.csv(paste0(DataFile,"/BC/BC_trawl_survey_sable_data.Oct312019.csv"))  %>%  names()
assc <- read.csv("./data/assessment_CPUE.csv") %>%
mutate(Source = 'assessment', Estimate_metric_tons = Value ,
Fleet2 = substr(Index,1,2)) %>%
filter(Type == 'Abundance' |Type == 'Biomass')  %>%
filter(!grepl('PUE', Index),!grepl('NUM', Index)) %>%
bind_rows(.,
read.csv(paste0(DataFile,"/BC/BC_sable_survey_data.Aug262019.csv"))  %>%
filter(START_LONGITUDE <= 0 & !is.na(CPUE_TRAPS) & !is.na(TOTAL_SABLE_WEIGHT) &
SABLE_SET_TYPE == 'StRS') %>% group_by(SET_YEAR) %>%
dplyr::summarise(meanCPUE = mean(cpue)) %>%
mutate(Year = SET_YEAR, Value = meanCPUE, CV = NA,
Index = 'Filter_StRS', Type = 'Biomass', Source = 'Assessment',
Estimate_metric_tons =meanCPUE, Fleet2 =
'BC') %>%
select(-meanCPUE, -SET_YEAR)) %>%
bind_rows(.,
read.csv(paste0(DataFile,"/BC/BC_trawl_survey_sable_data.Oct312019.csv"))  %>%
filter(LONGITUDE <= 0 & !is.na(CATCH_WEIGHT)) %>%
group_by(YEAR) %>%
dplyr::summarise(meanCPUE = mean(CATCH_WEIGHT)) %>%
mutate(Year = SET_YEAR, Value = meanCPUE, CV = NA,
Index = 'Filter_BCTrawl', Type = 'Biomass', Source = 'Assessment',
Estimate_metric_tons =meanCPUE, Fleet2 =
'BC') %>%
select(-meanCPUE, -SET_YEAR))
assc <- read.csv("./data/assessment_CPUE.csv") %>%
mutate(Source = 'assessment', Estimate_metric_tons = Value ,
Fleet2 = substr(Index,1,2)) %>%
filter(Type == 'Abundance' |Type == 'Biomass')  %>%
filter(!grepl('PUE', Index),!grepl('NUM', Index)) %>%
bind_rows(.,
read.csv(paste0(DataFile,"/BC/BC_sable_survey_data.Aug262019.csv"))  %>%
filter(START_LONGITUDE <= 0 & !is.na(CPUE_TRAPS) & !is.na(TOTAL_SABLE_WEIGHT) &
SABLE_SET_TYPE == 'StRS') %>% group_by(SET_YEAR) %>%
dplyr::summarise(meanCPUE = mean(cpue)) %>%
mutate(Year = SET_YEAR, Value = meanCPUE, CV = NA,
Index = 'Filter_StRS', Type = 'Biomass', Source = 'Assessment',
Estimate_metric_tons =meanCPUE, Fleet2 =
'BC') %>%
select(-meanCPUE, -SET_YEAR)) %>%
bind_rows(.,
read.csv(paste0(DataFile,"/BC/BC_trawl_survey_sable_data.Oct312019.csv"))  %>%
filter(LONGITUDE <= 0 & !is.na(CATCH_WEIGHT)) %>%
group_by(YEAR) %>%
dplyr::summarise(meanCPUE = mean(CATCH_WEIGHT)) %>%
mutate(Year = YEAR, Value = meanCPUE, CV = NA,
Index = 'Filter_BCTrawl', Type = 'Biomass', Source = 'Assessment',
Estimate_metric_tons =meanCPUE, Fleet2 =
'BC') %>%
select(-meanCPUE, -SET_YEAR))
assc <- read.csv("./data/assessment_CPUE.csv") %>%
mutate(Source = 'assessment', Estimate_metric_tons = Value ,
Fleet2 = substr(Index,1,2)) %>%
filter(Type == 'Abundance' |Type == 'Biomass')  %>%
filter(!grepl('PUE', Index),!grepl('NUM', Index)) %>%
bind_rows(.,
read.csv(paste0(DataFile,"/BC/BC_sable_survey_data.Aug262019.csv"))  %>%
filter(START_LONGITUDE <= 0 & !is.na(CPUE_TRAPS) & !is.na(TOTAL_SABLE_WEIGHT) &
SABLE_SET_TYPE == 'StRS') %>% group_by(SET_YEAR) %>%
dplyr::summarise(meanCPUE = mean(cpue)) %>%
mutate(Year = SET_YEAR, Value = meanCPUE, CV = NA,
Index = 'Filter_StRS', Type = 'Biomass', Source = 'Assessment',
Estimate_metric_tons =meanCPUE, Fleet2 =
'BC') %>%
select(-meanCPUE, -SET_YEAR)) %>%
bind_rows(.,
read.csv(paste0(DataFile,"/BC/BC_trawl_survey_sable_data.Oct312019.csv"))  %>%
filter(LONGITUDE <= 0 & !is.na(CATCH_WEIGHT)) %>%
group_by(YEAR) %>%
dplyr::summarise(meanCPUE = mean(CATCH_WEIGHT)) %>%
mutate(Year = YEAR, Value = meanCPUE, CV = NA,
Index = 'Filter_BCTrawl', Type = 'Biomass', Source = 'Assessment',
Estimate_metric_tons =meanCPUE, Fleet2 =
'BC') %>%
select(-meanCPUE, -YEAR))
## Correct scales
assc$Estimate_metric_tons[assc$Fleet2 == 'AK'] <- assc$Value[assc$Fleet2 == 'AK']*  1000
# assc$Estimate_metric_tons[assc$Fleet2 == 'WC'] <- assc$Value * 1000
assc$Estimate_metric_tons[assc$Fleet2 == 'BC'] <- assc$Value[assc$Fleet2 == 'BC'] * 1000
names(assc) <- c('Year','Value','SD_log',"Fleet","TYPE", 'Source',"Estimate_metric_tons","Fleet2")
assc <- assc %>%select(Year, Fleet, Estimate_metric_tons, SD_log, TYPE, Source, Fleet2 ) %>%
mutate(uci=NA, lci = NA)
## see line 81 here for conv https://github.com/James-Thorson-NOAA/FishStatsUtils/blob/master/R/plot_index.R
vastc <- read.csv(paste0(DateFile,"Table_for_SS3_original.csv")) %>%
mutate(TYPE = 'Abundance', Source = 'VAST',
lci = Estimate_metric_tons-SD_mt,
uci = Estimate_metric_tons+SD_mt) %>%
select(Year, Fleet, Estimate_metric_tons, SD_log, TYPE, Source, uci, lci ) %>%
filter(Fleet != 'Eastern_Bering_Sea')
## see line 81 here for conv https://github.com/James-Thorson-NOAA/FishStatsUtils/blob/master/R/plot_index.R
vastc <- read.csv(paste0(DateFile,"Table_for_SS3.csv")) %>%
mutate(TYPE = 'Abundance', Source = 'VAST',
lci = Estimate_metric_tons-SD_mt,
uci = Estimate_metric_tons+SD_mt) %>%
select(Year, Fleet, Estimate_metric_tons, SD_log, TYPE, Source, uci, lci ) %>%
filter(Fleet != 'Eastern_Bering_Sea')
vastc$Fleet2 <- NA
for(i in 1:nrow(vastc)){
vastc$Fleet2[i] <- ifelse(vastc$Fleet[i] == "California_current",
"WC",
ifelse(vastc$Fleet[i] == "British_Columbia", "BC",
"AK"))
if(vastc$Fleet[i] == 'All')   vastc$Fleet2[i] <- NA
}
rbind(vastc,assc) %>%
filter(Fleet2 %in% c('WC','BC')) %>%
filter(Fleet %in% c("California_current","British_Columbia","Gulf_of_Alaska","Eastern_Bering_Sea",
'AK_DOM_LL','AK_GOA_TRW','Filter_StRS','BC_TRAP_SRV_STRAT','WC_TRI_TRW','WC_SH_SLP_TRW')) %>%
ggplot(., aes(x = Year, y =Estimate_metric_tons, col = Fleet, linetype = Fleet)) +
theme_minimal()+
# theme(panel.grid.minor = element_blank()) +
scale_x_continuous(limits = c(1975,2020)) +
# scale_color_brewer(palette = 'Spectral') +
scale_color_manual(values = c(rep('blue',3),'gold2','gold','seagreen2','seagreen3','seagreen4','grey22','grey44'))+
scale_linetype_manual(values = c(rep('solid',3), rep('dashed',40))) +
labs(x = 'Year', y = 'Estimate (mt)',
title = 'Indices from VAST and Assessment, by region') +
geom_line(lwd = 0.9)+
geom_ribbon(aes(ymin = lci, ymax = uci), alpha = 0.2, col = 'grey') +
# geom_point(pch = 1, cex = 3) +
facet_wrap(~Fleet2, scales = 'free_y', ncol = 3)
rbind(vastc,assc) %>%
filter(Fleet2 %in% c('WC','BC')) %>%
filter(Fleet %in% c("California_current","British_Columbia","Gulf_of_Alaska","Eastern_Bering_Sea",
'AK_DOM_LL','AK_GOA_TRW',"Filter_BCTrawl", 'WC_TRI_TRW','WC_SH_SLP_TRW')) %>%
ggplot(., aes(x = Year, y =Estimate_metric_tons, col = Fleet, linetype = Fleet)) +
theme_minimal()+
# theme(panel.grid.minor = element_blank()) +
scale_x_continuous(limits = c(1975,2020)) +
# scale_color_brewer(palette = 'Spectral') +
scale_color_manual(values = c(rep('blue',3),'gold2','gold','seagreen2','seagreen3','seagreen4','grey22','grey44'))+
scale_linetype_manual(values = c(rep('solid',3), rep('dashed',40))) +
labs(x = 'Year', y = 'Estimate (mt)',
title = 'Indices from VAST and Assessment, by region') +
geom_line(lwd = 0.9)+
geom_ribbon(aes(ymin = lci, ymax = uci), alpha = 0.2, col = 'grey') +
# geom_point(pch = 1, cex = 3) +
facet_wrap(~Fleet2, scales = 'free_y', ncol = 3)
rm(list= ls())
qgamma(0.05,67,6)
hist(rgamma(10000,67,6))
hist(rgamma(10000,8,1), add = T)
hist(rgamma(10000,8,1), add = T, col = 'blue')
hist(rgamma(10000,67,6), xlim = c(0,20))
hist(rgamma(10000,8,1), add = T, col = 'blue')
